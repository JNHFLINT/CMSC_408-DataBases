---
title: Homework 1 - Open Source Tools
author:
    - name: Joseph Hughes
      email: hughesj5@vcu.edu
format:
    html:
        embed-resources: true
        html-math-method: katex
        theme: spacelab
        toc: true

## Useful references:

# - Basic Markdown: https://quarto.org/docs/authoring/markdown-basics.html
# - Quarto figures: https://quarto.org/docs/authoring/figures.html
# - HTML document basics: https://quarto.org/docs/output-formats/html-basics.html
# - Quarto guide: https://quarto.org/docs/guide/
# - VS Code and Quarto: https://quarto.org/docs/tools/vscode.html
#   (RTFM and GET THE EXTENSION!!!!)

---

The purpose of this report is to first give us an overview of how databases work. We first learn about the Open Source Data Engineering Tools which are used to create massive databases which store lots of data. We then learn about the different types of technical tools which are used to make it easier to create these databases. Lastly, we reflect on what we have learned and how we liked this report.

# Open Source Data Engineering Tools

Author Alireza Sadeghi offers a nice overview of the [2024 ata engineering landscape](https://practicaldataengineering.substack.com/p/open-source-data-engineering-landscape) in the on-line web site [Practical Data Engineering Substack](https://practicaldataengineering.substack.com/).

![](assets/tools-2024.webp)

# Major Categories

Mr. Sadeghi proposals nine major tools categories.

## Storage Systems

Storage systems are foundational components in data engineering that handle the storage and retrieval of data. These systems can include traditional databases, distributed storage solutions, and modern cloud-based storage options. They provide scalable, reliable, and secure environments to store structured, semi-structured, and unstructured data, ensuring data is accessible for processing, analysis, and other operations.

## Data Lake Platform

Data Lake platforms are designed to store vast amounts of raw data in its native format until it is needed. Unlike traditional databases, data lakes can store structured, semi-structured, and unstructured data, providing a flexible and scalable storage solution. These platforms support the integration of multiple data sources, making it easier to perform big data analytics and machine learning on large datasets.

## Data Integration

Data Integration involves combining data from different sources into a unified view, faciliating seamless access across various systems. It ensures that disparate data sets work together cohesively, enabling more comprehensive and accurate insights.

## Data Processing and Computation

Data Processing and Computation refer to the transformation, aggregation, and analysis of data, enabling the extraction of meaningful insights and the execution of complex algorithms. These processes are crucial for converting raw data into a format that can be easily analyzed and interpreted.

## Workflow and DataOps

Workflow and DataOps tools automate and streamline the processes involved in data engineering, ensuring efficient and reliable data pipelines from ingestion to deployment. They enhance collaboration between teams, reduce errors, and enable faster iteration cycles in data-driven projects.

## Data Infrastructure and Monitoring

Data Infrastructure and Monitoring tools manage the underlying architecture and continuously track the performance, health, and security of data systems. They play a vital role in maintaining system reliability, preventing downtime, and ensuring that data services run smoothly.

## ML/AI Platform

ML/AI Platforms provide the tools and frameworks necessary for developing, training, and deploying machine learning and artificial intelligence models at scale. These platforms often include features for model versioning, experiment tracking, and deployment, making it easier to manage the entire ML lifecycle.

## Metadata Management

Metadata Management tools organize, catalog, and govern data assets by maintaining detailed information about data origins, structure, and usage, ensuring proper data stewardship. Effective metadata management improves data discoverablity and enhances data quality by providing context and lineage information.

## Analytics and Visualization

Analytics and Visualization tools convert raw data into actionable insights through interactive reports, dashboards, and data visualizations, facilitating informed decision-making. These tools allow users to explore data trends, identify patterns, and communicate findings effectively across an organization.

# Digging into the details

In the following sections I identify three subcategories of data engineering tools of greatest interest to me.

## Relational OLTP DBMS

This section is provided as an example.  You MAY NOT submit *relational OLTP DBMS* as part of your project 1!

**Relational Online Transaction Processing (OLTP) Database Management Systems (DBMS)** are designed to manage and execute a large number of short, transactional operations that involve the insertion, update, and deletion of data in a relational database. These systems are optimized for environments where data integrity, consistency, and speed are critical due to the high volume of concurrent transactions.

### Unique Characteristics of Relational OLTP DBMS:

1. **Structured Data Storage:**
   - Data is stored in a structured format using tables that define specific schemas, with rows representing records and columns representing attributes of the data.
   - The use of primary and foreign keys enforces relationships between different tables, maintaining data integrity across the database.

2. **ACID Compliance:**
   - OLTP systems are designed to be ACID-compliant (Atomicity, Consistency, Isolation, Durability). This ensures that all transactions are processed reliably, even in the event of a system failure.
   - Atomicity guarantees that all parts of a transaction are completed; otherwise, the transaction is aborted.
   - Consistency ensures that each transaction brings the database from one valid state to another.
   - Isolation keeps transactions separated until they are completed, preventing concurrency issues.
   - Durability ensures that once a transaction is committed, it is permanently recorded in the database, even in the case of a power failure.

3. **Concurrency Control:**
   - OLTP systems employ sophisticated concurrency control mechanisms, such as locking and transaction isolation levels, to manage simultaneous data access by multiple users or processes.
   - These mechanisms prevent conflicts and ensure that transactions are executed in a consistent and reliable manner, even under heavy load.

4. **High Availability and Reliability:**
   - OLTP databases are designed for environments that require high availability and reliability. They often include features such as replication, clustering, and failover to ensure continuous operation even during hardware or network failures.
   - Backup and recovery mechanisms are also integral to these systems, ensuring that data can be restored to a consistent state in the event of a failure.

5. **Scalability:**
   - While traditionally OLTP systems were scaled vertically by adding more resources to a single server, modern OLTP DBMS solutions support horizontal scaling, enabling distributed transactions across multiple servers or nodes.
   - Distributed OLTP systems can handle a larger number of transactions by partitioning data and processing load across multiple instances.

### Usage of Relational OLTP DBMS:

1. **Business Applications:**
   
   OLTP systems are widely used in business applications that require the processing of large volumes of simple, repetitive transactions, such as order processing, inventory management, financial transactions, and customer relationship management (CRM) systems.
   
   Examples include e-commerce platforms, banking systems, and ERP (Enterprise Resource Planning) software.

2. **Real-Time Transaction Processing:**

   OLTP systems are ideal for applications requiring real-time data entry, updates, and querying, where the accuracy and speed of transactions directly impact business operations.

   They are often used in industries such as finance, telecommunications, retail, and healthcare, where real-time data processing is critical.

3. **Data Integrity and Consistency:**
   
   OLTP databases are crucial in environments where data integrity and consistency are non-negotiable. For example, in financial systems, it is vital that all transactions are recorded accurately and that balances reflect all processed transactions immediately.

4. **User-Driven Querying:**
   
   OLTP systems support user-driven querying, allowing end-users to retrieve and interact with data through applications in real-time. These queries are typically simple and involve accessing specific records or performing updates on existing data.

### Examples of Open-Source Relational OLTP DBMS:

**PostgreSQL:** A powerful, open-source relational database system that is known for its robustness, extensibility, and support for complex queries and transactions.

**MySQL:** A widely-used open-source relational database known for its ease of use, reliability, and performance, particularly in web-based applications.

**MariaDB:** A fork of MySQL, MariaDB is designed to be highly compatible with MySQL while offering additional features and performance improvements.

**SQLite:** A lightweight, embedded SQL database engine that is self-contained and requires minimal setup, commonly used in mobile apps and small to medium-sized applications.

These relational OLTP DBMS tools are foundational in many organizations, ensuring that transactional data is handled efficiently and reliably, forming the backbone of countless business operations worldwide.

## Data Infrastructure and Monitoring

**What is Data Infrastructure and Monitoring?:** Data infrastructure and monitoring refer to the systems, tools, and processes that manage, observe, and maintain the overall data ecosystem within an organization. This includes the architecture and technologies that support data collection, storage, transfer, integration, and quality assurance. Monitoring tools specifically focus on tracking the performance, reliability, and integrity of these data systems in real-time, providing alerts and insights to ensure smooth operation.

**Importance:** This category is crucial because it forms the backbone of any data-driven organization. Efficient data infrastructure ensures that data is available, reliable, and accessible when needed. Monitoring tools help in proactively identifying and resolving issues, thereby minimizing downtime, data loss, or corruption. Without robust infrastructure and monitoring, the integrity and availability of data could be compromised, leading to faulty decision-making and operational inefficiencies.

**Differences from Relational OLTP Databases:** Relational OLTP (Online Transaction Processing) databases focus on managing day-to-day transactional data with a high degree of normalization, integrity, and support for ACID properties (Atomicity, Consistency, Isolation, Durability). Data infrastructure, on the other hand, encompasses a broader scope, including data lakes, ETL (Extract, Transform, Load) pipelines, and data warehouses, which may include both structured and unstructured data, and typically handle large volumes of data for analytical purposes rather than just transactional.

**Why I find it interesting:** I find Data infrastructure and monitoring interesting because of how important they are for scalability and reliability of data systems. I'm particularly interested in cloud technologies, containerization, and automation to streamline data operations.

## Data Processing and Computation

**What is Data Processing and Computation?:** Data processing and computation involve the transformation, analysis, and computation of data to derive actionable insights or to prepare data for further use. This category includes tools and frameworks for batch processing, stream processing, big data analytics, machine learning workflows, and distributed computing.

**Importance:** Data processing and computation are essential for turning raw data into meaningful information. These processes enable organizations to analyze trends, make predictions, and support decision-making in real-time or near-real-time. The efficiency and scalability of data processing systems directly impact how quickly and effectively data-driven insights can be generated.

**Differences from Relational OLTP Databases:** While OLTP databases are optimized for managing transactions (inserts, updates, deletes), data processing and computation tools are designed for large-scale data aggregation, complex queries, and transformations that often span massive datasets. They also differ in their ability to handle parallel processing and distributed computing across multiple nodes, something that traditional OLTP databases are not typically designed for.

**Why I find it interesting:** I find Data Processing and Computation interesting because they're so many fascinating tools which have transformed how large-scale data processing is done. I like understanding how these programs have the ability to process and compute vast amounts of data efficiently.

## Storage Systems

**What are Storage Systems?:** Storage systems refer to the technologies and architectures used to store and manage data across various formats and media. This includes traditional databases, data lakes, data warehouses, object storage, distributed file systems, and emerging storage technologies like NVMe (Non-Volatile Memory Express) and SSDs (Solid State Drives).

**Importance:** Effective storage systems are vital for ensuring that data is stored securely, efficiently, and in a way that is accessible for processing and analysis. The choice of storage system affects data retrieval speeds, storage costs, and the ability to scale as data grows. It also impacts data governance, security, and compliance.

**Differences from Relational OLTP Databases:** Relational OLTP databases are typically concerned with the real-time storage of structured data with strict schema enforcement. In contrast, storage systems can handle a variety of data types (structured, semi-structured, unstructured) and are designed to support different access patterns and use cases, such as large-scale analytics or archival storage. They may also prioritize different aspects, such as redundancy, durability, or performance, depending on the use case.

**Why I find it interesting:** I find storage systems interesting because of how they are organized. There are so many different ways to organize data and with the latest technological advancements, they are both cost-effective and scalable.


# Reflection

Convert these questions into brief paragraph responses (two or three sentence).

* I really liked how it was formatted. Once I got everything setup, I was able to get through the homework efficiently.

* The hardest part I had with this project was my PATH environment variable for Visual Studio Code. In the end, I had to reinstall and manually add the PATH environment variable in my advanced system settings.

* What I found most surprising about this assignment was how many types of tools we can use to make databases and visualizations. I really enjoyed using Github, Visual Studio Code, and Quarto to help this homework come together.

* I would give myself more time next time. I did end up learning everything, but it would help me if I started earlier so I can fully learn how these tools work.
